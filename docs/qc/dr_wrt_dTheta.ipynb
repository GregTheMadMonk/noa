{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "latin-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dqc\n",
    "import dqc.xc\n",
    "import dqc.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sorted-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLDAX(dqc.xc.CustomXC):\n",
    "    def __init__(self, a, p):\n",
    "        super().__init__()\n",
    "        self.a = a\n",
    "        self.p = p\n",
    "        self.number_of_parameters = 2\n",
    "\n",
    "    @property\n",
    "    def family(self):\n",
    "        # 1 for LDA, 2 for GGA, 4 for MGGA\n",
    "        return 1\n",
    "\n",
    "    def get_edensityxc(self, densinfo):\n",
    "        # densinfo has up and down components\n",
    "        if isinstance(densinfo, dqc.utils.SpinParam):\n",
    "            # spin-scaling of the exchange energy\n",
    "            return 0.5 * (self.get_edensityxc(densinfo.u * 2) + self.get_edensityxc(densinfo.d * 2))\n",
    "        else:\n",
    "            rho = densinfo.value.abs() + 1e-15  # safeguarding from nan\n",
    "            return self.a * rho ** self.p\n",
    "        \n",
    "    def get_edensityxc_derivative(self, densinfo, parameter_number):\n",
    "        # densinfo has up and down components\n",
    "        if isinstance(densinfo, dqc.utils.SpinParam):\n",
    "            # spin-scaling of the exchange energy\n",
    "            return 0.5 * (self.get_edensityxc_derivative(densinfo.u * 2, number_of_parameter) \n",
    "                          + self.get_edensityxc_derivative(densinfo.d * 2, number_of_parameter))\n",
    "        else:\n",
    "            rho = densinfo.value.abs() + 1e-15  # safeguarding from nan\n",
    "            if parameter_number == 0: # parameter a\n",
    "                return rho ** self.p\n",
    "            elif parameter_number == 1: # parameter p\n",
    "                return self.a * rho ** (self.p - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "still-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.nn.Parameter(torch.tensor(1.0, dtype=torch.double))\n",
    "p = torch.nn.Parameter(torch.tensor(2.0, dtype=torch.double))\n",
    "myxc = MyLDAX(a, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "average-retro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4645, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mol = dqc.Mol(moldesc=\"H -1 0 0; H 1 0 0\", basis=\"3-21G\")\n",
    "qc = dqc.KS(mol, xc=myxc).run()\n",
    "ene = qc.energy()\n",
    "print(ene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "conscious-yellow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.3848e-31, -1.4465e-16, -6.4918e-31, -9.2522e-16],\n",
       "        [-1.4465e-16,  4.7720e-02,  2.1416e-16,  3.0523e-01],\n",
       "        [-6.4918e-31,  2.1416e-16,  9.6112e-31,  1.3698e-15],\n",
       "        [-9.2522e-16,  3.0523e-01,  1.3698e-15,  1.9523e+00]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = qc._dm.detach().clone() # density matrix\n",
    "dm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-livestock",
   "metadata": {},
   "source": [
    "We have two types of equations.\n",
    "First, orthonomality equations:\n",
    "$$ r_{a}(\\textbf{C}) = \\sum_i\\sum_jC_{ai}\\delta_{ij}C_{aj}- 1 = \\sum_iC_{ai}C_{ai}- 1 = 0$$\n",
    "Obviosly, they do not depend on $\\vec{\\theta}$, so\n",
    "$$\\frac{\\partial r_{a}(\\textbf{C})}{\\partial\\vec{\\theta}}= 0$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "velvet-resource",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.]])\n",
      "tensor([[0.]])\n"
     ]
    }
   ],
   "source": [
    "nao = qc.get_system().get_hamiltonian().nao\n",
    "norb = qc._engine.norb\n",
    "\n",
    "derivative_of_normalization_first = torch.zeros((1, norb))\n",
    "derivative_of_normalization_second = torch.zeros((1, norb))\n",
    "\n",
    "print(derivative_of_normalization_first)\n",
    "print(derivative_of_normalization_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-therapy",
   "metadata": {},
   "source": [
    "Second, Roothan equations:\n",
    "$$r_{ia}(\\textbf{C};\\;\\vec{\\theta}) = \\sum_j (F_{ij}[\\rho](\\vec{\\theta})C_{aj} - \\epsilon_aS_{ij}C_{aj}) = 0$$\n",
    "Derivative:\n",
    "$$\\frac{\\partial r_{ia}(\\textbf{C};\\;\\vec{\\theta})}{\\partial \\vec{\\theta}} =\n",
    "\\sum_j C_{aj}\\int b_i(\\vec{r}) \\frac{\\partial V_{XC}[\\rho](\\vec{r};\\;\\vec{\\theta})}{\\partial \\vec{\\theta}} b_j(\\vec{r})d\\vec{r}$$\n",
    "As we know, $$V_{XC}[\\rho](\\vec{r};\\;\\vec{\\theta}) = \\frac{\\partial E_{XC}[\\rho]}{\\partial\\rho(\\vec{r})} = \\frac{\\int \\rho(\\vec{r})\\epsilon_{XC}[\\rho](\\vec{r};\\;\\vec{\\theta})d\\vec{r}}{\\partial\\rho(\\vec{r})}$$\n",
    "So\n",
    "$$\\frac{\\partial r_{ia}(\\textbf{C};\\;\\vec{\\theta})}{\\partial \\vec{\\theta}} =\n",
    "\\sum_j C_{aj}\\int b_i(\\vec{r}) \\frac{\\int \\rho(\\vec{r})\\epsilon_{XC}[\\rho](\\vec{r};\\;\\vec{\\theta})d\\vec{r}}{\\partial \\vec{\\theta}\\partial\\rho(\\vec{r})} b_j(\\vec{r})d\\vec{r} = \n",
    "\\sum_j C_{aj}\\int b_i(\\vec{r}) \\frac{\\int \\rho(\\vec{r}) \\frac{\\partial \\epsilon_{XC}[\\rho](\\vec{r};\\;\\vec{\\theta})}{\\partial \\vec{\\theta}}d\\vec{r} }{\\partial\\rho(\\vec{r})} b_j(\\vec{r})d\\vec{r}\n",
    "$$\n",
    "It means, we can use $\\frac{\\partial \\epsilon_{XC}[\\rho](\\vec{r};\\;\\vec{\\theta})}{\\partial \\vec{\\theta}}$ instead of $\\epsilon_{XC}[\\rho](\\vec{r};\\;\\vec{\\theta})$ in DQC function`get_vxc()`and get suitable result. This function takes densinfo and, I guess, somehow takes functional derivative with respect to density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "listed-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqc.utils.datastruct import ValGrad, SpinParam\n",
    "\n",
    "def get_vxc_derivative(xc, densinfo, number_of_parameter):\n",
    "    \"\"\"\n",
    "    Returns the ValGrad for the xc potential given the density info\n",
    "    for unpolarized case.\n",
    "    \"\"\"\n",
    "    # This is the default implementation of vxc if there is no implementation\n",
    "    # in the specific class of XC.\n",
    "\n",
    "    # densinfo.value & lapl: (*BD, nr)\n",
    "    # densinfo.grad: (*BD, ndim, nr)\n",
    "    # return:\n",
    "    # potentialinfo.value & lapl: (*BD, nr)\n",
    "    # potentialinfo.grad: (*BD, ndim, nr)\n",
    "\n",
    "    # mark the densinfo components as requiring grads\n",
    "    with xc._enable_grad_densinfo(densinfo):\n",
    "        with torch.enable_grad():\n",
    "            edensity_derivative = xc.get_edensityxc_derivative(densinfo, number_of_parameter)  # (*BD, nr)\n",
    "        grad_outputs = torch.ones_like(edensity_derivative)\n",
    "        grad_enabled = torch.is_grad_enabled()\n",
    "\n",
    "        if not isinstance(densinfo, ValGrad):  # polarized case\n",
    "            if xc.family == 1:  # LDA\n",
    "                params = (densinfo.u.value, densinfo.d.value)\n",
    "                dedn_u, dedn_d = torch.autograd.grad(\n",
    "                    edensity_derivative, params, create_graph=grad_enabled, grad_outputs=grad_outputs)\n",
    "\n",
    "                return SpinParam(u=ValGrad(value=dedn_u), d=ValGrad(value=dedn_d))\n",
    "\n",
    "            elif xc.family == 2:  # GGA\n",
    "                params = (densinfo.u.value, densinfo.d.value, densinfo.u.grad, densinfo.d.grad)\n",
    "                dedn_u, dedn_d, dedg_u, dedg_d = torch.autograd.grad(\n",
    "                    edensity_derivative, params, create_graph=grad_enabled, grad_outputs=grad_outputs)\n",
    "\n",
    "                return SpinParam(\n",
    "                    u=ValGrad(value=dedn_u, grad=dedg_u),\n",
    "                    d=ValGrad(value=dedn_d, grad=dedg_d))\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    \"Default polarized vxc for family %s is not implemented\" % self.family)\n",
    "        else:  # unpolarized case\n",
    "            if xc.family == 1:  # LDA\n",
    "                dedn, = torch.autograd.grad(\n",
    "                    edensity_derivative, densinfo.value, create_graph=grad_enabled,\n",
    "                    grad_outputs=grad_outputs)\n",
    "\n",
    "                return ValGrad(value=dedn)\n",
    "\n",
    "            elif xc.family == 2:  # GGA\n",
    "                dedn, dedg = torch.autograd.grad(\n",
    "                    edensity_derivative, (densinfo.value, densinfo.grad), create_graph=grad_enabled,\n",
    "                    grad_outputs=grad_outputs)\n",
    "\n",
    "                return ValGrad(value=dedn, grad=dedg)\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError(\"Default vxc for family %d is not implemented\" % self.family)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e36a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamitonian_get_vxc_derivative(hamiltonian, dm, number_of_parameter):\n",
    "        # dm: (*BD, nao, nao)\n",
    "    assert hamiltonian.xc is not None, \"Please call .setup_grid with the xc object\"\n",
    "\n",
    "    densinfo = SpinParam.apply_fcn(\n",
    "        lambda dm_: hamiltonian._dm2densinfo(dm_), dm)  # value: (*BD, nr)\n",
    "    potinfo = get_vxc_derivative(hamiltonian.xc, densinfo, number_of_parameter) # value: (*BD, nr)\n",
    "    vxc_linop = SpinParam.apply_fcn(\n",
    "        lambda potinfo_: hamiltonian._get_vxc_from_potinfo(potinfo_), potinfo)\n",
    "    return vxc_linop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fiscal-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dm2fock_derivative(engine, dm, number_of_parameter):\n",
    "    vxc_derivative = hamitonian_get_vxc_derivative(engine.hamilton, dm, number_of_parameter)  # spin param or tensor (..., nao, nao)\n",
    "    return SpinParam.apply_fcn(lambda vxc_: vxc_, vxc_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "suspected-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any, List, Union, overload, Tuple\n",
    "from dqc.utils.datastruct import SpinParam\n",
    "\n",
    "def dm2scp_derivative(engine, dm: Union[torch.Tensor, SpinParam[torch.Tensor]], number_of_parameter) -> torch.Tensor:\n",
    "    # convert from density matrix to a self-consistent parameter (scp)\n",
    "    if isinstance(dm, torch.Tensor):  # unpolarized\n",
    "        # scp is the fock matrix\n",
    "        return dm2fock_derivative(engine, dm, number_of_parameter).fullmatrix()\n",
    "    else:  # polarized\n",
    "        # scp is the concatenated fock matrix\n",
    "        fock = self.dm2fock_derivative(dm)\n",
    "        mat_u = fock.u.fullmatrix().unsqueeze(0)\n",
    "        mat_d = fock.d.fullmatrix().unsqueeze(0)\n",
    "        return torch.cat((mat_u, mat_d), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "connected-ghana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.1203e-02, -1.8399e-16,  4.9318e-02,  9.4651e-17],\n",
       "        [-1.8399e-16,  8.3114e-02, -1.5266e-16, -5.9360e-02],\n",
       "        [ 4.9318e-02, -1.5266e-16,  1.1935e-01,  5.4644e-17],\n",
       "        [ 9.4651e-17, -5.9360e-02,  5.4644e-17,  8.9399e-02]],\n",
       "       dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fock_derivative_first = dm2scp_derivative(qc._engine, dm, 0)\n",
    "fock_derivative_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5178287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.9999e-01, -7.0777e-16,  3.0187e-06,  3.4694e-17],\n",
       "        [-7.0777e-16,  1.0000e+00, -5.3429e-16, -1.2964e-06],\n",
       "        [ 3.0187e-06, -5.3429e-16,  1.0000e+00, -1.2490e-16],\n",
       "        [ 3.4694e-17, -1.2964e-06, -1.2490e-16,  1.0000e+00]],\n",
       "       dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fock_derivative_second = dm2scp_derivative(qc._engine, dm, 1)\n",
    "fock_derivative_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e572f8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.6823e-16],\n",
       "        [-1.5447e-01],\n",
       "        [-6.9322e-16],\n",
       "        [-9.8800e-01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff = qc.get_system().get_hamiltonian().dm2ao_orb_params(dm, norb) # full C matrix\n",
    "coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b3e4e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.9990e-17,  4.5810e-02, -9.0049e-17, -7.9157e-02]],\n",
       "       dtype=torch.float64, grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equation_derivative_wrt_first = torch.matmul(coeff.t(), fock_derivative_first)\n",
    "equation_derivative_wrt_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bcf6b3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.4328e-16, -1.5447e-01, -4.8729e-16, -9.8800e-01]],\n",
       "       dtype=torch.float64, grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equation_derivative_wrt_second = torch.matmul(coeff.t(), fock_derivative_second)\n",
    "equation_derivative_wrt_second"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
